{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "499c0bf4",
   "metadata": {},
   "source": [
    "# Most Used Functions in Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e9392b",
   "metadata": {},
   "source": [
    "Scrapy is an open-source and collaborative web crawling framework for Python. It is used to extract data from websites. In this notebook, we will cover some of the most commonly used functions and techniques in Scrapy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1b383a",
   "metadata": {},
   "source": [
    "## 1. Creating a Scrapy Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad0b18",
   "metadata": {},
   "source": [
    "To start a new Scrapy project, use the `startproject` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79cbed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to create a new Scrapy project\n",
    "!scrapy startproject myproject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01103fe",
   "metadata": {},
   "source": [
    "## 2. Creating a Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e8da8",
   "metadata": {},
   "source": [
    "A spider is a class that you define and that Scrapy uses to scrape information from a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f36807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Spider code\n",
    "import scrapy\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"myspider\"\n",
    "    start_urls = ['http://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').get()\n",
    "        yield {'title': title}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3421c0",
   "metadata": {},
   "source": [
    "## 3. Running a Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75edfb0c",
   "metadata": {},
   "source": [
    "To run a spider, use the `crawl` command followed by the name of the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd2187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to run a spider\n",
    "!scrapy crawl myspider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716ce7b5",
   "metadata": {},
   "source": [
    "## 4. Selecting Elements with CSS Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b23061",
   "metadata": {},
   "source": [
    "Scrapy provides a powerful selection mechanism using CSS selectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51896bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using CSS selectors\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"myspider\"\n",
    "    start_urls = ['http://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for item in response.css('div.item'):\n",
    "            title = item.css('h2::text').get()\n",
    "            yield {'title': title}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fbcd3f",
   "metadata": {},
   "source": [
    "## 5. Selecting Elements with XPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a280c45",
   "metadata": {},
   "source": [
    "XPath is another powerful way to select elements from a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89258238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using XPath selectors\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"myspider\"\n",
    "    start_urls = ['http://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for item in response.xpath('//div[@class=\"item\"]'):\n",
    "            title = item.xpath('h2/text()').get()\n",
    "            yield {'title': title}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1614363",
   "metadata": {},
   "source": [
    "## 6. Following Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c84bcc5",
   "metadata": {},
   "source": [
    "To follow links, use the `follow` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of following links\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"myspider\"\n",
    "    start_urls = ['http://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for href in response.css('a::attr(href)'):\n",
    "            yield response.follow(href, self.parse_detail)\n",
    "\n",
    "    def parse_detail(self, response):\n",
    "        title = response.css('title::text').get()\n",
    "        yield {'title': title}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b93bd",
   "metadata": {},
   "source": [
    "## 7. Storing the Scraped Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374588cd",
   "metadata": {},
   "source": [
    "Scrapy can store the scraped data in various formats such as JSON, CSV, and XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e3cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to store the scraped data in JSON format\n",
    "!scrapy crawl myspider -o output.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863c19c6",
   "metadata": {},
   "source": [
    "## 8. Handling Request and Response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7392025c",
   "metadata": {},
   "source": [
    "Scrapy allows you to handle requests and responses efficiently using middleware and custom handlers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb718eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of handling requests and responses\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"myspider\"\n",
    "    start_urls = ['http://example.com']\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://example.com/page1',\n",
    "            'http://example.com/page2',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').get()\n",
    "        yield {'title': title}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c9a0e9",
   "metadata": {},
   "source": [
    "## 9. Using Scrapy Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c6116",
   "metadata": {},
   "source": [
    "Scrapy Shell is an interactive shell that allows you to test your scraping code in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f118da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to open Scrapy Shell\n",
    "!scrapy shell 'http://example.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed48899a",
   "metadata": {},
   "source": [
    "## 10. Using Scrapy Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe720f",
   "metadata": {},
   "source": [
    "Scrapy Pipelines are used to process the scraped data after it has been extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c81a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a Scrapy Pipeline\n",
    "class MyPipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        item['title'] = item['title'].upper()\n",
    "        return item"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
